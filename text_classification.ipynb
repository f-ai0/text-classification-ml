{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baffec70-6412-43ef-b45e-d1adbda4619e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Complete. Starting Training\n",
      "\n",
      "Data Preparation Complete. Running 3 Scenarios\n"
     ]
    }
   ],
   "source": [
    "# Necessary Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Download NLTK stopwords (if not already done)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Custom Algorithm 1: Multinomial Naive Bayes (Vectorized)\n",
    "class CustomMultinomialNB:\n",
    "    \"\"\"Multinomial Naive Bayes implemented with maximum Vectorization.\"\"\"\n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # 1. Priors (Vectorized): Calculate Log(P(C))\n",
    "        self.priors = np.array([np.log(np.sum(y == c) / N) for c in self.classes])\n",
    "        \n",
    "        # 2. Likelihoods (Vectorized): Calculate Log(P(W|C))\n",
    "        alpha = 1.0 \n",
    "        self.likelihoods_log = [] \n",
    "        \n",
    "        for c_idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c] \n",
    "            word_counts = np.sum(X_c, axis=0) \n",
    "            total_words_c = np.sum(word_counts) \n",
    "            \n",
    "            log_likelihood = np.log((word_counts + alpha) / (total_words_c + alpha * D))\n",
    "            self.likelihoods_log.append(log_likelihood)\n",
    "\n",
    "        self.likelihoods_log = np.array(self.likelihoods_log)\n",
    "\n",
    "\n",
    "    def predict(self, X):        \n",
    "        # Log-Posterior = Log(P(C)) + Sum(Log(P(W | C)))\n",
    "        # Likelihood Sum: np.dot(X, Likelihoods.T)\n",
    "        log_likelihood_matrix = np.dot(X, self.likelihoods_log.T)        \n",
    "        \n",
    "        # Add Priors: (Broadcasting)        \n",
    "        log_posteriors = log_likelihood_matrix + self.priors\n",
    "        \n",
    "        # Final Prediction\n",
    "        return self.classes[np.argmax(log_posteriors, axis=1)]\n",
    "\n",
    "# Custom Algorithm 2: Linear SVM (Batch Gradient Descent - Vectorized)\n",
    "class CustomLinearSVM_BGD:\n",
    "    \"\"\"Linear SVM Classifier implemented using Batch Gradient Descent (BGD).\"\"\"\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=100): \n",
    "        self.lr = learning_rate \n",
    "        self.lambda_param = lambda_param \n",
    "        self.n_iters = n_iters \n",
    "        self.W = None \n",
    "        self.b = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "        # Convert labels to {-1, 1}\n",
    "        y_ = np.where(y <= 0, -1, 1) \n",
    "        self.W = np.zeros(D)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            # 1. Calculate Margins (Vectorized): y_i * (W . x_i + b)\n",
    "            linear_output = np.dot(X, self.W) + self.b \n",
    "            margins = y_ * linear_output             \n",
    "            \n",
    "            # 2. Identify violations: samples where margin < 1\n",
    "            misclassified_mask = margins < 1           \n",
    "            \n",
    "            # 3. Calculate Weight Gradient (Vectorized):\n",
    "            \n",
    "            # 3.1. Loss Gradient Term: -y_i * x_i (for violations only)\n",
    "            loss_gradient_part = np.zeros(X.shape)\n",
    "            loss_gradient_part[misclassified_mask] = -y_[misclassified_mask, None] * X[misclassified_mask]\n",
    "            \n",
    "            # 3.2. Average Loss Gradient over all N samples (Sum over N)\n",
    "            total_loss_gradient = np.sum(loss_gradient_part, axis=0) / N\n",
    "            \n",
    "            # 3.3. Regularization Gradient (L2): 2 * lambda * W\n",
    "            reg_gradient = 2 * self.lambda_param * self.W\n",
    "            \n",
    "            # 4. Full Update for Weights (W):\n",
    "            full_gradient_W = reg_gradient + total_loss_gradient\n",
    "            self.W -= self.lr * full_gradient_W\n",
    "            \n",
    "            # 5. Bias Update (b):\n",
    "            bias_gradient = -np.sum(y_[misclassified_mask]) / N\n",
    "            self.b -= self.lr * bias_gradient\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.W) + self.b\n",
    "        return np.where(linear_output <= 0, 0, 1)\n",
    "\n",
    "# Data Loading, Preprocessing, and Vectorization\n",
    "try:\n",
    "    df = pd.read_csv(\"spam.csv\", encoding='iso-8859-1')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'spam.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "df = df.iloc[:, :2]\n",
    "df.columns = ['label', 'text']\n",
    "df = df.dropna()\n",
    "df['label'] = df['label'].map({'ham':0, 'spam':1})\n",
    "y = df['label'].values\n",
    "\n",
    "def advanced_clean_text(text):\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text) # Remove non-ASCII\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "df['text'] = df['text'].apply(advanced_clean_text)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data Preparation Complete. Starting Training\\n\")\n",
    "\n",
    "# Function to Run a Single Scenario (Splitting, Training, Evaluation)\n",
    "def run_scenario_and_collect(test_size_ratio, scenario_tag):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size_ratio, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # --- Train and Evaluate Naive Bayes ---\n",
    "    start_time_nb = time.time()\n",
    "    nb_model = CustomMultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    runtime_nb = (time.time() - start_time_nb) * 1000\n",
    "    y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "    # --- Train and Evaluate Linear SVM (BGD) ---\n",
    "    start_time_svm = time.time()\n",
    "    svm_model = CustomLinearSVM_BGD()\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    runtime_svm = (time.time() - start_time_svm) * 1000\n",
    "    y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "    # Collect metrics for Naive Bayes\n",
    "    nb_metrics = {\n",
    "        'Algorithm': 'Naive Bayes (MNB)',\n",
    "        f'Accuracy_{scenario_tag}': accuracy_score(y_test, y_pred_nb),\n",
    "        f'Precision_{scenario_tag}': precision_score(y_test, y_pred_nb, zero_division=0),\n",
    "        f'Recall_{scenario_tag}': recall_score(y_test, y_pred_nb),\n",
    "        f'F1-Score_{scenario_tag}': f1_score(y_test, y_pred_nb, zero_division=0),\n",
    "        f'Time_{scenario_tag}': runtime_nb\n",
    "    }\n",
    "\n",
    "    # Collect metrics for SVM\n",
    "    svm_metrics = {\n",
    "        'Algorithm': 'Support Vector Machine (SVM)',\n",
    "        f'Accuracy_{scenario_tag}': accuracy_score(y_test, y_pred_svm),\n",
    "        f'Precision_{scenario_tag}': precision_score(y_test, y_pred_svm, zero_division=0),\n",
    "        f'Recall_{scenario_tag}': recall_score(y_test, y_pred_svm),\n",
    "        f'F1-Score_{scenario_tag}': f1_score(y_test, y_pred_svm, zero_division=0),\n",
    "        f'Time_{scenario_tag}': runtime_svm\n",
    "    }\n",
    "    \n",
    "    return [nb_metrics, svm_metrics]\n",
    "    \n",
    "# Run the Three Scenarios and Merge Results\n",
    "print(\"Data Preparation Complete. Running 3 Scenarios\")\n",
    "\n",
    "# Scenario Definitions\n",
    "SCENARIOS = [\n",
    "    {'ratio': 1/3, 'tag': '1/3T'}, # 1/3 Test, 2/3 Train\n",
    "    {'ratio': 2/3, 'tag': '2/3T'}, # 2/3 Test, 1/3 Train\n",
    "    {'ratio': 0.01, 'tag': '1%T'}  # 1% Test (Max Training Data)\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for scenario in SCENARIOS:\n",
    "    all_results.extend(run_scenario_and_collect(scenario['ratio'], scenario['tag']))\n",
    "\n",
    "# Convert to DataFrame and Merge Horizontally\n",
    "results_df = pd.DataFrame(all_results)\n",
    "final_results = results_df.groupby('Algorithm').first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333ff53c-99c5-4bbf-8773-8eb62c2036a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Performance Evaluation (From Scratch) - Multi-Scenario Comparison\n",
      "\n",
      "             Algorithm              | 2/3 Train (1/3 Test) | 1/3 Train (2/3 Test) | 99% Train (1% Test)\n",
      "                                    |    Acc     Time(s)   |    Acc     Time(s)   |    Acc     Time(s)   \n",
      "Naive Bayes (MNB)                   |    0.9612  523.11    |    0.9254  131.56    |    0.9464  484.45   \n",
      "Support Vector Machine (SVM)        |    0.8660 93175.07   |    0.8659 51031.71   |    0.8571 130913.80 \n"
     ]
    }
   ],
   "source": [
    "# 7. Final Output (Horizontal/Wide Format)\n",
    "print(\"Algorithm Performance Evaluation (From Scratch) - Multi-Scenario Comparison\\n\")\n",
    "\n",
    "# Build the custom header\n",
    "header_line_1 = \"{:^35} | {:^17} | {:^17} | {:^17}\".format(\"Algorithm\", \"2/3 Train (1/3 Test)\", \"1/3 Train (2/3 Test)\", \"99% Train (1% Test)\")\n",
    "header_line_2 = \"{:^35} | {:^9} {:^10} | {:^9} {:^10} | {:^9} {:^10} \".format(\n",
    "    \"\", \"Acc\", \"Time(s)\", \"Acc\", \"Time(s)\", \"Acc\", \"Time(s)\")\n",
    "\n",
    "print(header_line_1)\n",
    "print(header_line_2)\n",
    "\n",
    "# Print the data rows\n",
    "for index, row in final_results.iterrows():\n",
    "    alg = row['Algorithm']\n",
    "    \n",
    "    def format_metrics_row_simple(tag):\n",
    "        # Format: Acc | Time(s)\n",
    "        return \"{:8.4f} {:^8.2f}\".format(\n",
    "            row[f'Accuracy_{tag}'], row[f'Time_{tag}'])\n",
    "    \n",
    "    s1_data = format_metrics_row_simple('1/3T') # 1/3 Test\n",
    "    s2_data = format_metrics_row_simple('2/3T') # 2/3 Test\n",
    "    s3_data = format_metrics_row_simple('1%T')  # 1% Test (Max Train)\n",
    "    \n",
    "    # Print the row with best possible alignment\n",
    "    print(\"{:<35} | {:^20} | {:^20} | {:^20}\".format(alg, s1_data, s2_data, s3_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f9e1034-7ad5-4757-8722-cebf1fc297e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Algorithm          Case  Training Time (s)\n",
      "0  Naive Bayes     Best Case             0.0406\n",
      "1  Naive Bayes  Average Case             0.0050\n",
      "2  Naive Bayes    Worst Case             0.0060\n",
      "3          SVM     Best Case             0.0293\n",
      "4          SVM  Average Case             0.0152\n",
      "5          SVM    Worst Case             0.0112\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "spam = [\"Free offer now\"]*5000\n",
    "ham = [\"Hello, how are you doing today?\"]*5000\n",
    "messages = spam + ham  \n",
    "labels = [1]*5000 + [0]*5000  # 1 = Spam, 0 = Ham\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(messages)\n",
    "y = labels\n",
    "\n",
    "cases = {\n",
    "    \"Best Case\": vstack([X[:500], X[5000:5500]]),       # 500 Spam + 500 Ham\n",
    "    \"Average Case\": vstack([X[:2500], X[5000:7500]]),  # 2500 Spam + 2500 Ham\n",
    "    \"Worst Case\": X                                    \n",
    "}\n",
    "labels_cases = {\n",
    "    \"Best Case\": y[:500] + y[5000:5500],\n",
    "    \"Average Case\": y[:2500] + y[5000:7500],\n",
    "    \"Worst Case\": y\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Naive Bayes\n",
    "for case in cases:\n",
    "    start = time.time()\n",
    "    model = MultinomialNB()\n",
    "    model.fit(cases[case], labels_cases[case])\n",
    "    end = time.time()\n",
    "    results.append([\"Naive Bayes\", case, round(end - start, 4)])\n",
    "\n",
    "# SVM\n",
    "for case in cases:\n",
    "    start = time.time()\n",
    "    model = SVC(kernel='linear')  # Linear SVM\n",
    "    model.fit(cases[case], labels_cases[case])\n",
    "    end = time.time()\n",
    "    results.append([\"SVM\", case, round(end - start, 4)])\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"Algorithm\", \"Case\", \"Training Time (s)\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc644f7-d67c-4e66-94d8-8cea1eebbdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
